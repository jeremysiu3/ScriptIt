{"pageid": 1519776, "title": "শিখন হার", "url": "https://bn.wikipedia.org/wiki?curid=1519776", "content": "মেশিন লার্নিংয়ে শিখন হার(ইংরেজিঃ Learning rate)  হলো এমন একটি টিউনিং প্যারামিটার যা নির্ধারণ করে একটি অ্যালগরিদম প্রতিবার কতটা এগোবে ক্ষতি ফাংশনের সর্বনিম্ন বিন্দুর দিকে। এটি নতুন তথ্য পুরানো তথ্যকে কতটা প্রভাবিত করবে তা ঠিক করে, অর্থাৎ মডেল কত দ্রুত \"শিখবে\"। অ্যাডাপটিভ কন্ট্রোলে একে গেইন বলে।\nশিখন হার ঠিক করতে গিয়ে গতি আর ওভারশুটিংয়ের মধ্যে ভারসাম্য রাখতে হয়। গ্রেডিয়েন্ট দিক দেখায়, কিন্তু শিখন হার ধাপের আকার ঠিক করে। বেশি হলে সর্বনিম্ন ছাড়িয়ে যাবে, কম হলে অনেক সময় লাগবে বা ভুল জায়গায় আটকে যাবে।\nশিখন হার প্রশিক্ষণের সময় বদলানো হয় যাতে দ্রুত কনভারজেন্স হয়, দোলন না হয় এবং ভুল স্থানীয় সর্বনিম্নে আটকে না যায়। এটি একটি নির্দিষ্ট সিডিউল বা অ্যাডাপটিভ পদ্ধতি দিয়ে করা হয়।  প্রতিটি প্যারামিটারের জন্য শিখন হার আলাদা হতে পারে, তখন এটি একটি তির্যক ম্যাট্রিক্সের মতো হয়, যা নিউটনের পদ্ধতিতে হেসিয়ান ম্যাট্রিক্সের বিপরীতের আনুমানিক হিসেবে বোঝা যায়। এটি কোয়াসি-নিউটন পদ্ধতি এবং অন্যান্য অপ্টিমাইজেশন অ্যালগরিদমে অসম্পূর্ণ লাইন সার্চের ধাপের দৈর্ঘ্যের সঙ্গে জড়িত।\n\n\n== শিখন হার সিডিউল ==\nশিখন হার শুরুতে ডিফল্ট রাখা যায় বা কৌশল দিয়ে বেছে নেওয়া যায়। শিখন হার সিডিউল শিক্ষার সময় এটিকে বদলায়, সাধারণত এপক বা পুনরাবৃত্তির সঙ্গে।  এটি ক্ষয় আর গতি নামে দুটি জিনিস দিয়ে করা হয়। সাধারণ সিডিউলগুলো হলো সময়-ভিত্তিক, ধাপ-ভিত্তিক আর সূচকীয়।\nক্ষয় শিক্ষাকে সঠিক জায়গায় স্থির রাখে আর দোলন আটকায়। উচ্চ শিখন হার থাকলে শিক্ষা সর্বনিম্ন বিন্দুর ওপর দিয়ে লাফাতে পারে। এটি একটি হাইপারপ্যারামিটার দিয়ে নিয়ন্ত্রণ করা হয়।\nগতি একটি পাহাড়ে গড়ানো বলের মতো। আমরা চাই বলটি সবচেয়ে নিচে (কম ত্রুটির জায়গায়) থামুক। গতি শিক্ষার গতি বাড়ায় যখন ত্রুটির দিক একই থাকে, আর ছোট বাধা পার করে স্থানীয় সর্বনিম্ন এড়ায়। এটিও একটি হাইপারপ্যারামিটার দিয়ে নিয়ন্ত্রিত, যা বলের ভরের মতো। বেশি হলে বল সর্বনিম্ন ছাড়িয়ে যায়, কম হলে কাজ করে না। গতির সূত্র ক্ষয়ের চেয়ে জটিল, কিন্তু Keras-এর মতো লাইব্রেরিতে এটি আগে থেকে থাকে।\nসময়-ভিত্তিক লার্নিং সিডিউল পূর্ববর্তী সময়ের পুনরাবৃত্তির শিখন হারের উপর নির্ভর করে শিখন হার পরিবর্তন করে। ক্ষয় ফ্যাক্টরিংয়ের গাণিতিক সূত্রটি হল:\n\n  \n    \n      \n        \n          η\n          \n            n\n            +\n            1\n          \n        \n        =\n        \n          \n            \n              η\n              \n                n\n              \n            \n            \n              1\n              +\n              d\n              n\n            \n          \n        \n      \n    \n    {\\displaystyle \\eta _{n+1}={\\frac {\\eta _{n}}{1+dn}}}\n  \n\nযেখানে \n  \n    \n      \n        η\n      \n    \n    {\\displaystyle \\eta }\n  \n হল শিখন হার, \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n হল একটি ক্ষয় প্যারামিটার এবং \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n হল পুনরাবৃত্তি ধাপ।\nধাপ-ভিত্তিক লার্নিং সিডিউল কিছু পূর্বনির্ধারিত ধাপ অনুসারে শিখন হার পরিবর্তন করে। এখানে ক্ষয় প্রয়োগের সূত্রটি নিম্নরূপ:\n\n  \n    \n      \n        \n          η\n          \n            n\n          \n        \n        =\n        \n          η\n          \n            0\n          \n        \n        \n          d\n          \n            \n              ⌊\n              \n                \n                  \n                    1\n                    +\n                    n\n                  \n                  r\n                \n              \n              ⌋\n            \n          \n        \n      \n    \n    {\\displaystyle \\eta _{n}=\\eta _{0}d^{\\left\\lfloor {\\frac {1+n}{r}}\\right\\rfloor }}\n  \n\nযেখানে \n  \n    \n      \n        \n          η\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\eta _{n}}\n  \n হল পুনরাবৃত্তি \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-এ শিখন হার, \n  \n    \n      \n        \n          η\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\eta _{0}}\n  \n হল প্রাথমিক শিখন হার, \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n হল শিখন হার কতটা পরিবর্তন করা উচিত প্রতিটি ড্রপে (০.৫ অর্ধেক করার সাথে সম্পর্কিত) এবং \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n হল ড্রপ রেট, বা রেট কত ঘন ঘন কমানো উচিত (১০ প্রতি ১০ পুনরাবৃত্তিতে একটি ড্রপের সাথে সম্পর্কিত)। এখানে ফ্লোর ফাংশন (\n  \n    \n      \n        ⌊\n        …\n        ⌋\n      \n    \n    {\\displaystyle \\lfloor \\dots \\rfloor }\n  \n) তার ইনপুটের মানকে ১-এর চেয়ে ছোট সব মানের জন্য ০-তে নামিয়ে দেয়।\nএক্সপোনেনশিয়াল লার্নিং সিডিউল ধাপ-ভিত্তিকের মতো, তবে ধাপের পরিবর্তে একটি কমে যাওয়া এক্সপোনেনশিয়াল ফাংশন ব্যবহৃত হয়। ক্ষয় ফ্যাক্টরিংয়ের গাণিতিক সূত্রটি হল:\n\n  \n    \n      \n        \n          η\n          \n            n\n          \n        \n        =\n        \n          η\n          \n            0\n          \n        \n        \n          e\n          \n            −\n            d\n            n\n          \n        \n      \n    \n    {\\displaystyle \\eta _{n}=\\eta _{0}e^{-dn}}\n  \n\nযেখানে \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n হল একটি ক্ষয় প্যারামিটার।\n\n\n== অ্যাডাপটিভ শিখন হার ==\nশিখন হার সিডিউলের সমস্যা হলো এগুলো হাইপারপ্যারামিটারের উপর নির্ভর করে, যেগুলো প্রতিটি শিক্ষা সেশনের জন্য হাতে বেছে নিতে হয়। এই হাইপারপ্যারামিটারগুলো সমস্যা বা ব্যবহৃত মডেলের উপর ভিত্তি করে অনেকটা পরিবর্তিত হতে পারে। এই সমস্যা সমাধানের জন্য, বিভিন্ন অ্যাডাপটিভ গ্রেডিয়েন্ট ডিসেন্ট অ্যালগরিদম আছে, যেমন অ্যাডাগ্রাড, অ্যাডাডেল্টা, আরএমএসপ্রপ, এবং অ্যাডাম যেগুলো Keras-এর মতো লাইব্রেরিতে আগে থেকে থাকে।\n\n\n== আরও দেখুন ==\n\n\n== তথ্যসূত্র ==\n\n\n== আরও পড়ার জন্য ==\nGéron, Aurélien (২০১৭)। \"গ্রেডিয়েন্ট ডিসেন্ট\"। স্কিকিট-লার্ন এবং টেনসরফ্লো দিয়ে হ্যান্ডস-অন মেশিন লার্নিং। ও'রেইলি। পৃষ্ঠা 113–124। আইএসবিএন 978-1-4919-6229-9। \nPlagianakos, V. P.; Magoulas, G. D.; Vrahatis, M. N. (২০০১)। \"স্টোকাস্টিক গ্রেডিয়েন্ট ডিসেন্টে লার্নিং রেট অ্যাডাপটেশন\"। কনভেক্স বিশ্লেষণ এবং গ্লোবাল অপ্টিমাইজেশনে অগ্রগতি। ক্লুয়ার। পৃষ্ঠা 433–444। আইএসবিএন 0-7923-6942-4। \n\n\n== বহিঃসংযোগ ==\nde Freitas, Nando (১২ ফেব্রুয়ারি ২০১৫)। \"অপ্টিমাইজেশন\"। ডিপ লার্নিং লেকচার ৬। অক্সফোর্ড বিশ্ববিদ্যালয়  – ইউটিউব-এর মাধ্যমে।"}