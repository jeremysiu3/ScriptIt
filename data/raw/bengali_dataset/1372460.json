{"pageid": 1372460, "title": "জিপিটি-৩", "url": "https://bn.wikipedia.org/wiki?curid=1372460", "content": "জেনারেটিভ প্রি-ট্রেইন্ড ট্রান্সফরমার ৩ (জিপিটি-৩) হল ২০২০ সালে প্রকাশিত একটি অটোরিগ্রেসিভ ল্যাঙ্গুয়েজ মডেল, যা গভীর শিখন ব্যবহার করে মানুষের মতো লেখা রচনা করতে পারে। প্রণোদনা হিসাবে এটিতে একটি প্রাথমিক লেখা দেওয়া হলে, এটি নতুন লেখা তৈরি করে প্রদত্ত লেখাটিকে বড় করতে থাকবে।\nজিপিটি-৩ গঠিত হয়েছে একটি ২০৮৬-টোকেন-লম্বা ডিকোডার-অনলি ট্রান্সফরমার নেটওয়ার্ক প্রসঙ্গ ও ১৭৫ বিলিয়ন প্যারামিটারের মাধ্যমে, যেটি সংরক্ষণ করতে ৮০০ গিগাবাইট কম্পিউটার মেমোরির প্রয়োজন হয়। মডেলটি জেনারেটিভ প্রি-ট্রেনিং ব্যবহার করে প্রশিক্ষিত করা হয়েছে যা পূর্ববর্তী টোকেনগুলির (শব্দ) উপর ভিত্তি করে পরবর্তী টোকেন কী হবে তার পূর্বাভাস দিতে পারে। মডেলটি অনেক কাজে শক্তিশালী জিরো-শট এবং ফিউ-শট শিক্ষন প্রদর্শন করেছে।\nজিপিটি সিরিজে জিপিটি-৩ হল জিপিটি-২ এর তৃতীয় প্রজন্মের ভাষা পূর্বাভাস মডেলের উত্তরসূরি। এটি তৈরি করেছে যুক্তরাষ্ট্রের সান ফ্রানসিস্কোতে অবস্থিত ওপেনএআই নামক একটি কৃত্রিম বুদ্ধিমত্তা গবেষণা কেন্দ্র। ২০২০ সালের মে মাসে জিপিটি-৩ জনসম্মুখে উন্মুক্ত করা হয় এবং জুলাই ২০২০ পর্যন্ত এটি বিটা পর্যায়ের নিরীক্ষায় ছিল। জিপিটি-৩ বিশ্বব্যাপী চলমান স্বাভাবিক ভাষা প্রক্রিয়াজাতকরণ গবেষণার একটি অংশ যা প্রাক-প্রশিক্ষিত ভাষা উপস্থাপনার একটি সাম্প্রতিক প্রবণতা।\nজিপিটি-৩ এত নিখুতভাবে লেখা রচনা করতে পারে যে এটির লেখা রচনা আর মানুষের লেখা রচনার মধ্যে পার্থক্য নিরূপণ করাটা বেশ কঠিন। এই অবস্থার উপকারিতা এবং ঝুঁকি উভয়ই আছে। ওপেনএআইয়ের ৩১ জন গবেষক ও প্রকৌশলী ২৮ মে ২০২০ সালে জিপিটি-৩ এর উপর তাদের গবেষণা প্রতিবেদন প্রকাশ করে। গবেষণাপত্রটিতে তারা জিপিটি-৩ এর সম্ভাব্য ঝুঁকি সম্পর্কে সতর্ক করে এ সম্পর্কে আরও গবেষণার আহ্বান জানায়।  একজন অস্ট্রেলিয়ান দার্শনিক ডেভিড চালমারস জিপিটি-৩ কে \"এখন পর্যন্ত উদ্ভাবিত সবচেয়ে আকর্ষণীয় এবং গুরুত্বপূর্ণ কৃত্রিম বুদ্ধিমত্তা সিস্টেমগুলির মধ্যে একটি\" হিসাবে মতামত ব্যক্ত করেছেন। দ্য নিউ ইয়র্ক টাইমস- এর এপ্রিল ২০২২ সালের একটি পর্যালোচনায় জিপিটি-৩-কে মানুষের মত সাবলিলতায় রচনা লিখতে সক্ষম বলে উল্লেখ করা হয়।\nমাইক্রোসফট ২২ সেপ্টেম্বর ২০২০ সালের একটি ঘোষণার মাধ্যমে জানায় যে, তাঁরা জিপিটি-৩ এর \"একচেটিয়া\" ব্যবহারের অনুমোদন অর্জন করেছে; অন্যরা জিপিটি-৩ এর উন্মুক্ত এপিআই ব্যবহার করতে পারলেও কেবলমাত্র মাইক্রোসফটের কাছে জিপিটি-৩ এর অভ্যন্তরীণ মডেলের প্রবেশাধিকার থাকবে।\n\n\n== পটভূমি ==\nদ্য ইকোনমিস্ট- এর মতে, উন্নত অ্যালগরিদম, শক্তিশালী কম্পিউটার এবং ডিজিটাইজড ডেটার বৃদ্ধি মেশিন লার্নিং- এ একটি বিপ্লব ঘটিয়েছে, ২০১০-এর দশকে নতুন কৌশল উদ্ভাবনের ফলে ভাষাকে প্রভাবিত করার কাজে \"দ্রুত উন্নতি\" হয়েছে। অনেকটা মানুষের মস্তিষ্কের নিউরাল নেটওয়ার্কের মত সফটওয়্যার মডেলগুলোকে হাজার হাজার কিংবা লক্ষ লক্ষ উদাহরনের মাধ্যমে প্রশিক্ষণ প্রদান করা হয়। স্বাভাবিক ভাষা প্রক্রিয়াজাতকরণ (এনএলপি) এ ব্যবহৃত একটি আর্কিটেকচার হল একটি গভীর শিখন মডেলের উপর ভিত্তি করে একটি নিউরাল নেটওয়ার্ক যা ২০১৭ সালে প্রথম চালু করা হয়। এর নাম ট্রান্সফরমার  জিপিটি-এন মডেল হল ট্রান্সফরমার-ভিত্তিক একটি গভীর শিখন নিউরাল নেটওয়ার্ক আর্কিটেকচার। ভাষা প্রক্রিয়াকরন, মাইনিং, বিন্যাস, ইনপুটের মধ্যে সংযোগ সাধন ও বৈপরীত্য খুঁজে বের করতে সক্ষম এবং সেইসাথে সঠিকভাবে প্রশ্নের উত্তর দিতে সক্ষম এমন অনেক এনএলপি ব্যবস্থা বর্তমানে আছে।\n১১ জুন ২০১৮ সালে ওপেনএআইয়ের গবেষক ও প্রকৌশলীরা একটি গবেষণা প্রতিবেদন প্রকাশ করেন যেখানে প্রথমবারের মত জেনারেটিভ প্রি-ট্রেইন্ড ট্রান্সফরমার (জিপিটি)'কে উপস্থাপন করা হয়। জিপিটি একটি উৎপাদনশীল বৃহৎ ভাষা মডেল যেটিকে বৈচিত্র্যময় ও বিপুল পরিমাণ লেখ্য তথ্য দ্বারা প্রশিক্ষিত করা হয়। পরবর্তীতে ওপেনএইআইয়ের গবেষক ও প্রকৌশলীরা জিপিটি দ্বারা নির্দিষ্ট কাজ সম্পন্ন করার জন্য বৈষম্যমূলক ফাইন-টিউনিং সম্পন্ন করেন। জিপিটি মডেলগুলো হলো গভীর শিখনের মাধ্যমে তৈরি ট্রান্সফরমার ভিত্তিক প্রাক-প্রশিক্ষিত নিউরাল নেটওয়ার্ক। তবে সবচেয়ে ভাল কাজ করা স্বাভাবিক ভাষা মডেলগুলো সাধারণত তত্বাবধানাধীন শিখন প্রয়োগ করে বানানো হয় যেখানে বিপুল পরিমাণ হাতে চিহ্নিত তথ্য ব্যবহার করা হয়। ফলে এভাবে তৈরি করা ভাষা মডেলগুলো বানাতে ব্যাপক সময় ও সম্পদের প্রয়োজন হয়। প্রথম জিপিটি মডেলটির নাম ছিল \"জিপিটি-১\" এবং পরবর্তী জিপিটি মডেলটির নাম ছিল \"জিপিট-২\" যেটি ২০১৯ সালের ফেব্রুয়ারি মাসে প্রকাশিত হয়। জিপিটি-২ ছিল জিপিটি-১ এর একটি বর্ধিত রূপ। এটি জিপিটি-১ এর ডাটাসেট ও প্যারামিটারের সংখ্যার তুলনায় ১০ গুন বড় ছিল। ১৫০ কোটি প্যারামিটার সম্পন্ন এই মডেলটিকে ৮০ লক্ষ ওয়েব পাতার একটি ডাটাসেটের মাধ্যমে প্রশিক্ষণ প্রদান করা হয়েছিল।\n২০২০ সালের ফেব্রুয়ারি মাসে মাইক্রোসফট, টুরিন ন্যাচারাল ল্যাঙ্গুয়েজ জেনারেশন (টি-এনএলজি) নামে একটি বৃহৎ ভাষা মডেল প্রকাশ করে যেটিকে তারা \"বিশ্বের ইতিহাসে ১৭০ কোটি প্যারামিটার সমৃদ্ধ সবচেয়ে বড় ভাষা মডেল\" হিসেবে দাবি করে। এটি স্বয়ংক্রিয় সংক্ষিপ্তকরণ ও প্রশ্ন-উত্তর প্রদানসহ বিভিন্ন কাজে অন্যান্য ভাষা মডেলসমূহের চেয়ে ভাল কাজ করতে পারে।\n\n\n== প্রশিক্ষণ এবং ক্ষমতা ==\n\n28 মে, 2020-এ, OpenAI-এর 31 জন প্রকৌশলী এবং গবেষকদের একটি গ্রুপের একটি arXiv প্রিপ্রিন্ট GPT-3-এর বিকাশের বর্ণনা দিয়েছে, একটি তৃতীয় প্রজন্মের \"অত্যাধুনিক ভাষা মডেল\"।  দলটি GPT-3-এর ক্ষমতা তার পূর্বসূরি, GPT-2-এর থেকে দুই অর্ডারের বেশি মাত্রায় বাড়িয়েছে, যা GPT-3কে এখন পর্যন্ত সবচেয়ে বড় নন-স্পার্স ল্যাঙ্গুয়েজ মডেল বানিয়েছে।   যেহেতু GPT-3 কাঠামোগতভাবে এর পূর্বসূরীদের মতো,  এর বৃহত্তর নির্ভুলতা এর বর্ধিত ক্ষমতা এবং বৃহত্তর সংখ্যক পরামিতির জন্য দায়ী। GPT-3 এর ক্ষমতা মাইক্রোসফ্টের টুরিং NLG এর চেয়ে দশগুণ বড়, যা সেই সময়ে পরিচিত পরবর্তী বৃহত্তম NLP মডেল।\nLambdalabs অনুমান করেছে প্রায় $4.6 মিলিয়ন মার্কিন ডলার এবং 355 বছর একটি একক GPU- তে GPT-3 প্রশিক্ষণের জন্য 2020,  সমান্তরালভাবে আরও GPU ব্যবহার করে প্রকৃত প্রশিক্ষণের সময় কম।\nGPT-3 এর জন্য ওয়েটেড প্রাক-প্রশিক্ষণ ডেটাসেটের ষাট শতাংশ 410 বিলিয়ন বাইট-পেয়ার-এনকোডেড টোকেন সমন্বিত কমন ক্রলের একটি ফিল্টার করা সংস্করণ থেকে আসে।  অন্যান্য উত্স হল WebText2 থেকে 19 বিলিয়ন টোকেন যা মোট ওজনের 22% প্রতিনিধিত্ব করে, 8% প্রতিনিধিত্ব করে Books1 থেকে 12 বিলিয়ন টোকেন, 8% প্রতিনিধিত্ব করে Books2 থেকে 55 বিলিয়ন টোকেন এবং 3% প্রতিনিধিত্ব করে উইকিপিডিয়া থেকে 3 বিলিয়ন টোকেন।  GPT-3 শত শত বিলিয়ন শব্দের উপর প্রশিক্ষিত ছিল এবং এটি CSS, JSX, এবং Python এর মধ্যে কোডিং করতেও সক্ষম।\n\nযেহেতু GPT-3-এর প্রশিক্ষণের ডেটা সর্বাঙ্গীণ ছিল, তাই আলাদা ভাষার কাজের জন্য আরও প্রশিক্ষণের প্রয়োজন নেই। প্রশিক্ষণের ডেটা মাঝে মাঝে বিষাক্ত ভাষা ধারণ করে এবং GPT-3 মাঝে মাঝে বিষাক্ত ভাষা তৈরি করে তার প্রশিক্ষণের ডেটা নকল করার ফলে। ওয়াশিংটন বিশ্ববিদ্যালয়ের একটি সমীক্ষায় দেখা গেছে যে GPT-3 জিপিটি-2 এবং CTRL-এর অনুরূপ প্রাকৃতিক ভাষা প্রক্রিয়াকরণ মডেলের সাথে তুলনীয় বিষাক্ত স্তরে বিষাক্ত ভাষা তৈরি করেছে। OpenAI GPT-3 দ্বারা উত্পন্ন বিষাক্ত ভাষার পরিমাণ সীমিত করার জন্য বেশ কিছু কৌশল প্রয়োগ করেছে। ফলস্বরূপ, GPT-3 তার পূর্বসূরি মডেল, GPT-1-এর তুলনায় কম বিষাক্ত ভাষা তৈরি করেছে, যদিও এটি CTRL উইকির তুলনায় আরও প্রজন্ম এবং বিষাক্ত ভাষার উচ্চ বিষাক্ততা উভয়ই তৈরি করেছে, একটি ভাষা মডেল যা সম্পূর্ণরূপে উইকিপিডিয়া ডেটাতে প্রশিক্ষিত।\n11 জুন, 2020-এ, OpenAI ঘোষণা করেছে যে ব্যবহারকারীরা তার ব্যবহারকারী-বান্ধব GPT-3 API-এ অ্যাক্সেসের জন্য অনুরোধ করতে পারে—একটি \"মেশিন লার্নিং টুলসেট\" — যাতে OpenAI-কে এই নতুন প্রযুক্তির \"শক্তি ও সীমাগুলি অন্বেষণ করতে\" সাহায্য করা যায়। আমন্ত্রণটি বর্ণনা করেছে যে কীভাবে এই API-এর একটি সাধারণ-উদ্দেশ্য \"টেক্সট ইন, টেক্সট আউট\" ইন্টারফেস ছিল যা সাধারণ একক ব্যবহারের পরিবর্তে প্রায় \"যেকোন ইংরেজি ভাষার কাজ\" সম্পূর্ণ করতে পারে। একজন ব্যবহারকারীর মতে, যাদের OpenAI GPT-3 API-এর একটি প্রাইভেট প্রারম্ভিক রিলিজ অ্যাক্সেস ছিল, GPT-3 শুধুমাত্র কয়েকটি সহজ প্রম্পট সহ \"আশ্চর্যজনকভাবে সুসংগত পাঠ্য\" লেখার ক্ষেত্রে \"খুব ভালো\" ছিল। একটি প্রাথমিক পরীক্ষায় 80টি মার্কিন বিষয়কে বিচার করতে বলা হয়েছিল যে ~200 শব্দের সংক্ষিপ্ত নিবন্ধগুলি মানুষ বা GPT-3 দ্বারা লেখা হয়েছে কিনা। অংশগ্রহণকারীরা 52% সময় সঠিকভাবে বিচার করেছে, এলোমেলো অনুমান করার চেয়ে সামান্য ভাল করছে।\n18 নভেম্বর, 2021-এ, OpenAI ঘোষণা করেছে যে যথেষ্ট সুরক্ষা কার্যকর করা হয়েছে যে তার API-এ অ্যাক্সেস সীমাবদ্ধ থাকবে না। OpenAI ডেভেলপারদের একটি বিষয়বস্তু সংযম সরঞ্জাম প্রদান করেছে যা তাদেরকে OpenAI এর বিষয়বস্তু নীতি মেনে চলতে সাহায্য করে। 27 জানুয়ারী, 2022-এ, ওপেনএআই ঘোষণা করেছে যে তার নতুন GPT-3 ভাষার মডেলগুলি, সম্মিলিতভাবে InstructGPT নামে পরিচিত, এখন তাদের API- এ ব্যবহৃত ডিফল্ট ভাষা মডেল। OpenAI-এর মতে, InstructGPT এমন বিষয়বস্তু তৈরি করেছে যা ব্যবহারকারীর অভিপ্রায়ের সাথে আরও ভালভাবে সারিবদ্ধভাবে নির্দেশাবলী অনুসরণ করে, কম তৈরি করা তথ্য তৈরি করে এবং কিছুটা কম বিষাক্ত সামগ্রী তৈরি করে।\nযেহেতু GPT-3 \"সংবাদ নিবন্ধ তৈরি করতে পারে যা মানব মূল্যায়নকারীদের মানুষের দ্বারা লিখিত নিবন্ধগুলি থেকে আলাদা করতে অসুবিধা হয়,\" GPT-3 এর \"ভাষা মডেলের উপকারী এবং ক্ষতিকারক উভয় প্রয়োগকে এগিয়ে নেওয়ার সম্ভাবনা রয়েছে।\"  তাদের 28 মে, 2020 গবেষণাপত্রে, গবেষকরা সম্ভাব্য \"GPT-3 এর ক্ষতিকর প্রভাব\" বিস্তারিতভাবে বর্ণনা করেছেন যার মধ্যে রয়েছে \"ভুল তথ্য, স্প্যাম, ফিশিং, আইনি ও সরকারী প্রক্রিয়ার অপব্যবহার, প্রতারণামূলক একাডেমিক প্রবন্ধ লেখা এবং সামাজিক প্রকৌশল অজুহাত \"  লেখক ঝুঁকি প্রশমনে গবেষণার আহ্বান জানাতে এই বিপদগুলির প্রতি দৃষ্টি আকর্ষণ করেন।  \n2022 সালের জুন মাসে, আলমিরা ওসমানোভিক থানস্ট্রোম লিখেছিলেন যে GPT-3 নিজেই একটি নিবন্ধের প্রাথমিক লেখক, যে তারা এটি প্রকাশের জন্য জমা দিয়েছিল, এবং এটির পর্যালোচনা শেষ হওয়ার অপেক্ষায় এটি পূর্ব-প্রকাশিত হয়েছিল।\n\n\n=== GPT-3.5 ===\n15 মার্চ, 2022-এ, OpenAI তার API-এ GPT-3 এবং কোডেক্সের নতুন সংস্করণ \"টেক্সট-ডেভিন্সি-002\" এবং \"কোড-ডেভিন্সি-002\" নামে সম্পাদনা ও সন্নিবেশ করার ক্ষমতা সহ উপলব্ধ করেছে। এই মডেলগুলিকে পূর্ববর্তী সংস্করণগুলির তুলনায় আরও বেশি সক্ষম হিসাবে বর্ণনা করা হয়েছিল এবং জুন 2021 পর্যন্ত ডেটা সম্পর্কে প্রশিক্ষণ দেওয়া হয়েছিল 30 নভেম্বর, 2022-এ, OpenAI এই মডেলগুলিকে \"GPT-3.5\" সিরিজের অন্তর্গত হিসাবে উল্লেখ করা শুরু করে, এবং ChatGPT প্রকাশ করে, যেটি GPT-3.5 সিরিজের একটি মডেল থেকে সূক্ষ্ম সুর করা হয়েছিল।\n\n\n== অভ্যর্থনা ==\n\n\n=== প্রয়োগ ক্ষেত্র ===\nGPT-3, বিশেষ করে কোডেক্স মডেল, হল GitHub Copilot- এর ভিত্তি, একটি কোড কমপ্লিশন এবং জেনারেশন সফটওয়্যার যা বিভিন্ন কোড এডিটর এবং IDE-তে ব্যবহার করা যেতে পারে।\nGPT-3 কিছু নির্দিষ্ট Microsoft পণ্যে প্রচলিত ভাষাকে আনুষ্ঠানিক কম্পিউটার কোডে অনুবাদ করতে ব্যবহৃত হয়।\nএসকিউএল প্রসেসিংয়ের জন্য কোয়েরি-নির্দিষ্ট কোড তৈরি করতে কোডেক্সডিবি -এ GPT-3 ব্যবহার করা হয়েছে।\nGPT-3 জেসন রোহরার \"প্রজেক্ট ডিসেম্বর\" নামে একটি রেট্রো-থিমযুক্ত চ্যাটবট প্রকল্পে ব্যবহার করেছেন, যা অনলাইনে অ্যাক্সেসযোগ্য এবং ব্যবহারকারীদের GPT-3 প্রযুক্তি ব্যবহার করে বেশ কয়েকটি AI-এর সাথে কথোপকথন করতে দেয়।\nজিপিটি-৩ দ্য গার্ডিয়ান দ্বারা AI মানুষের জন্য ক্ষতিকারক নয় এমন একটি নিবন্ধ লেখার জন্য ব্যবহার করা হয়েছিল। এটিকে কিছু ধারণা দেওয়া হয়েছিল এবং আটটি ভিন্ন প্রবন্ধ তৈরি করা হয়েছিল, যা শেষ পর্যন্ত একটি নিবন্ধে একত্রিত হয়েছিল।\nAI Dungeon- এ GPT-3 ব্যবহার করা হয়েছিল, যা পাঠ্য-ভিত্তিক অ্যাডভেঞ্চার গেম তৈরি করে। পরবর্তীতে ওপেনএআই জেনারেট করা বিষয়বস্তু সংক্রান্ত তাদের নীতি পরিবর্তন করার পর এটি একটি প্রতিযোগী মডেল দ্বারা প্রতিস্থাপিত হয়।\nGPT-3 অনুলিপি এবং অন্যান্য বিপণন সামগ্রী লিখতে সহায়তা করতে ব্যবহৃত হয়।\nড্রেক্সেল ইউনিভার্সিটির 2022 সালের একটি গবেষণায় পরামর্শ দেওয়া হয়েছে যে আলঝেইমার রোগের প্রাথমিক লক্ষণগুলির জন্য GPT-3-ভিত্তিক সিস্টেমগুলি ব্যবহার করা যেতে পারে।\n\n\n=== মূল্যায়ন ===\nদ্য নিউ ইয়র্ক টাইমস- এ জুলাই 2020-এর একটি পর্যালোচনায়, ফরহাদ মঞ্জু বলেছিলেন যে GPT-3-এর কম্পিউটার কোড, কবিতা এবং গদ্য তৈরি করার ক্ষমতা শুধুমাত্র \"আশ্চর্যজনক\", \"ভয়ঙ্কর\" এবং \"নম্রকর\" নয়, বরং \"একটি থেকেও বেশি সামান্য ভয়ঙ্কর\"।\nডেইলি নউস GPT-3-তে নয়জন দার্শনিকের নিবন্ধের একটি সিরিজ উপস্থাপন করেছে। অস্ট্রেলিয়ান দার্শনিক ডেভিড চালমারস GPT-3 কে \"এখন পর্যন্ত উত্পাদিত সবচেয়ে আকর্ষণীয় এবং গুরুত্বপূর্ণ AI সিস্টেমগুলির মধ্যে একটি\" হিসাবে বর্ণনা করেছেন।\nওয়্যারড -এ একটি পর্যালোচনায় বলা হয়েছে যে GPT-3 \" সিলিকন ভ্যালি জুড়ে ঠান্ডা লাগার কারণ\"।\nন্যাশনাল ল রিভিউ বলেছে যে GPT-3 হল \"বৃহত্তর প্রক্রিয়ার একটি চিত্তাকর্ষক পদক্ষেপ\", যেখানে OpenAI এবং অন্যরা \"আরও সাধারণ বুদ্ধিমত্তার দিকে কাজ\" চালিয়ে যাওয়ার সময় \"এই সমস্ত শক্তির জন্য দরকারী অ্যাপ্লিকেশন\" খুঁজে পেয়েছে।\nএমআইটি টেকনোলজি রিভিউ- তে একটি নিবন্ধ, ডিপ লার্নিং সমালোচক গ্যারি মার্কাস দ্বারা লিখিত, বলেছে যে GPT-3-এর \"পৃথিবীর বোধগম্য প্রায়শই গুরুতরভাবে বন্ধ থাকে, যার মানে আপনি কখনই এটি যা বলে তা বিশ্বাস করতে পারবেন না।\" লেখকদের মতে, GPT-3 প্রতিটি শব্দের পেছনের অর্থ না বুঝেই শব্দের মধ্যে সম্পর্ক তৈরি করে।\nফেসবুক এআই ল্যাবের প্রধান জেরোম পেসেন্টি বলেন, জিপিটি-৩ \"অনিরাপদ\", ইহুদি, নারী, কৃষ্ণাঙ্গ মানুষদের নিয়ে আলোচনা করতে বলা হলে সিস্টেমের দ্বারা উত্পন্ন লিঙ্গবাদী, বর্ণবাদী এবং অন্যান্য পক্ষপাতদুষ্ট ও নেতিবাচক ভাষার দিকে ইঙ্গিত করে। হলোকাস্ট\nNabla, স্বাস্থ্যসেবা প্রযুক্তিতে বিশেষজ্ঞ একটি ফরাসি স্টার্ট-আপ, মেডিকেল চ্যাটবট হিসাবে GPT-3 পরীক্ষা করেছে, যদিও OpenAI নিজেই এই ধরনের ব্যবহারের বিরুদ্ধে সতর্ক করেছে। প্রত্যাশিত হিসাবে, GPT-3 বেশ কয়েকটি সীমাবদ্ধতা দেখিয়েছে। উদাহরণস্বরূপ, মানসিক স্বাস্থ্য সমস্যা সম্পর্কে GPT-3 প্রতিক্রিয়া পরীক্ষা করার সময়, AI একজন সিমুলেটেড রোগীকে আত্মহত্যা করার পরামর্শ দিয়েছে।\nনোয়াম চমস্কি GPT-3 এর বৈজ্ঞানিক মান সম্পর্কে তার সংশয় প্রকাশ করেছেন: \"এটি একটি ভাষার মডেল নয়। এটি বাস্তব ভাষার মতো অসম্ভব ভাষার জন্যও কাজ করে। তাই সাধারণ বৈজ্ঞানিক মাপকাঠি দ্বারা এটিকে একটি ভাষার মডেল হিসেবে অভিপ্রেত হলে তা খণ্ডন করা হয়। [. . . ] সম্ভবত এটি কিছু উদ্দেশ্যে উপযোগী, কিন্তু এটি সাধারণত ভাষা বা জ্ঞান সম্পর্কে আমাদের কিছুই বলে না বলে মনে হয়।\"\nলুসিয়ানো ফ্লোরিডি এবং ম্যাসিমো চিরিয়াত্তি \"ভাল, শব্দার্থিক নিদর্শনগুলির সস্তা উত্পাদন\" এর ঝুঁকি তুলে ধরেন।\nওপেনএআই-এর স্যাম অল্টম্যান নিজেই সমালোচনা করেছেন যাকে তিনি \"GPT-3 হাইপ\" বলেছেন, স্বীকার করেছেন GPT-3 \"এর গুরুতর দুর্বলতা রয়েছে এবং কখনও কখনও খুব বোকা ভুল করে। . . AI বিশ্বকে বদলে দিতে চলেছে, কিন্তু GPT-3 হল একটি খুব প্রাথমিক আভাস।\"\n\n\n=== সমালোচনা ===\nGPT-3 এর নির্মাতা, OpenAI, প্রাথমিকভাবে 2015 সালে একটি অলাভজনক হিসাবে প্রতিষ্ঠিত হয়েছিল 2019 সালে, ওপেনএআই তার স্বাভাবিক ওপেন-সোর্স স্ট্যান্ডার্ডগুলি থেকে GPT-3-এর পূর্বসূরি মডেল প্রকাশ না করে, উদ্বেগ উদ্ধৃত করে যে মডেলটি ভুয়া খবর প্রচারের সুবিধা দিতে পারে। OpenAI অবশেষে GPT-2 এর একটি সংস্করণ প্রকাশ করেছে যা আসল মডেলের আকারের 8% ছিল। একই বছরে, OpenAI একটি লাভজনক কোম্পানিতে পুনর্গঠন করে। 2020 সালে, মাইক্রোসফ্ট ঘোষণা করেছিল যে OpenAI-তে মাল্টি-বিলিয়ন ডলার বিনিয়োগের পরে মাইক্রোসফ্টের পণ্য এবং পরিষেবাগুলির জন্য কোম্পানির কাছে GPT-3 এর একচেটিয়া লাইসেন্স রয়েছে। চুক্তিটি OpenAI-কে একটি পাবলিক-ফেসিং API অফার করার অনুমতি দেয় যাতে ব্যবহারকারীরা মডেলের আউটপুট পেতে GPT-3 তে পাঠ্য পাঠাতে পারে, কিন্তু শুধুমাত্র Microsoft-এর কাছে GPT-3 এর সোর্স কোড অ্যাক্সেস থাকবে।\nGPT-3-এর মতো বড় ভাষার মডেলগুলি প্রশিক্ষণ এবং মডেলগুলি সংরক্ষণের পরিবেশগত প্রভাবের জন্য Google-এর কিছু AI নীতিশাস্ত্র গবেষকদের সমালোচনার মুখে পড়েছে, 2021 সালে Timnit Gebru এবং Emily M. Bender দ্বারা সহ-লেখক একটি গবেষণাপত্রে বিস্তারিত\nক্রমবর্ধমান </link> GPT-3 এবং অন্যান্য ভাষা জেনারেটরের উপর ভিত্তি করে স্বয়ংক্রিয় লেখার প্রযুক্তির ব্যবহার, একাডেমিক অখণ্ডতা সম্পর্কে উদ্বেগ উত্থাপন করেছে এবং কীভাবে বিশ্ববিদ্যালয় এবং স্কুলগুলি একাডেমিক অসদাচরণ যেমন চুরির মতো বিষয়গুলি নির্ধারণ করবে তা নির্ধারণ করবে।\nOpenAI-এর GPT সিরিজটি 12 বছর ধরে 60 মিলিয়ন ডোমেন থেকে স্ক্র্যাপ করা কপিরাইটযুক্ত নিবন্ধ, ইন্টারনেট পোস্ট, ওয়েব পেজ এবং বইগুলির একটি সমষ্টি, কমন ক্রল ডেটাসেট থেকে ডেটা নিয়ে তৈরি করা হয়েছিল। TechCrunch রিপোর্ট করে যে এই প্রশিক্ষণের ডেটাতে BBC, The New York Times, Reddit, অনলাইন বইগুলির সম্পূর্ণ পাঠ্য এবং আরও অনেক কিছু থেকে কপিরাইটযুক্ত উপাদান রয়েছে৷ ইউনাইটেড স্টেটস পেটেন্ট অ্যান্ড ট্রেডমার্ক অফিস (ইউএসপিটিও) থেকে কৃত্রিম বুদ্ধিমত্তা উদ্ভাবনের জন্য বুদ্ধিবৃত্তিক সম্পত্তি সুরক্ষার উপর মন্তব্যের জন্য 2019 সালের একটি অনুরোধের প্রতিক্রিয়ায়, OpenAI যুক্তি দিয়েছিল যে \"বর্তমান আইনের অধীনে, AI সিস্টেমের প্রশিক্ষণ [যেমন এর GPT মডেলগুলি] ন্যায্য ব্যবহার গঠন করে \"কিন্তু এটি \"বিন্দুতে মামলা আইনের অভাবের কারণে, ওপেনএআই এবং আমাদের মতো অন্যান্য এআই বিকাশকারীরা যথেষ্ট আইনি অনিশ্চয়তা এবং কমপ্লায়েন্স খরচের সম্মুখীন হচ্ছে।\"\n\n\n== আরও দেখুন ==\nল্যামডা\n\n\n== তথ্যসূত্র =="}