{"pageid": 1493390, "title": "জিপিটি-১", "url": "https://bn.wikipedia.org/wiki?curid=1493390", "content": "জেনারেটিভ প্রি-ট্রেইনড ট্রান্সফরমার ১ (Generative Pre-trained Transformer 1) ২০১৭ সালে গুগলের ট্রান্সফরমার স্থাপত্য আবিষ্কারের পর ওপেনএআইয়ের বৃহৎ ভাষার মডেলগুলির মধ্যে প্রথম ছিল। ২০১৮ সালের জুনে, ওপেনএআই \"জেনারেটিভ প্রাক-প্রশিক্ষণ দ্বারা ভাষা বোঝার উন্নতি\" শিরোনামের একটি গবেষণাপত্র প্রকাশ করে, যেখানে তারা একটি জেনারেটিভ প্রাক-প্রশিক্ষিত ট্রান্সফরমারের সাধারণ ধারণার সাথে সেই প্রাথমিক মডেলটি প্রবর্তন করে।\nএ পর্যন্ত সর্বোত্তম কার্যকারিতা সম্পন্ন নিউরাল এনএলপি মডেলগুলি মূলত ব্যাপক পরিমাণে ম্যানুয়ালি লেবেলকৃত ডেটা থেকে তত্ত্বীয় শিক্ষা ব্যবহার করত। এই সুপারভাইজড লার্নিংয়ের উপর নির্ভরতা তাদেরকে এমন ডেটাসেট ব্যবহার করতে সীমাবদ্ধ করে দেয় যেগুলির ভালভাবে অ্যানোটেট করা নেই, পাশাপাশি অত্যন্ত বড় মডেলগুলো প্রশিক্ষণ দিতে তা অত্যন্ত ব্যয়বহুল ও সময়সাপেক্ষ হয়ে ওঠে; অনেক ভাষা (যেমন সোয়াহিলি বা হাইতীয় ক্রেওল) এমন মডেলগুলির মাধ্যমে অনুবাদ এবং ব্যাখ্যা করা কঠিন, কারণ কোরপাস তৈরির জন্য পর্যাপ্ত টেক্সট নেই। তুলনামূলকভাবে, একটি জিপিটির \"সেমি-সুপারভাইজড\" পদ্ধতিতে দুটি পর্যায় অন্তর্ভুক্ত ছিল: একটি অসুপারভাইজড জেনারেটিভ \"প্রি-ট্রেইনিং\" পর্যায়, যেখানে একটি ভাষা মডেলিং উদ্দেশ্য ব্যবহার করে প্রাথমিক প্যারামিটার সেট করা হয়, এবং একটি সুপারভাইজড ডিসক্রিমিনেটিভ \"ফাইন-টিউনিং\" পর্যায়, যেখানে এই প্যারামিটারগুলি একটি লক্ষ্য কাজের জন্য অভিযোজিত হয়।\nএকটি ট্রান্সফরমার স্থাপত্যের ব্যবহার, মনোযোগ-বর্ধিত আরএনএন-এর সাথে জড়িত পূর্ববর্তী কৌশলগুলির বিপরীতে, জিপিটি মডেলগুলিকে পুনরাবৃত্ত প্রক্রিয়ার মাধ্যমে অর্জন করা যেতে পারে এমন আরও কাঠামোগত মেমোরি প্রদান করে; এর ফলে \"বিভিন্ন কাজসমূহে শক্তিশালী স্থানান্তর কর্মক্ষমতা\" বৃদ্ধি পায়।\n\n\n== বুককর্পাস বেছে নেওয়ার কারণ ==\nবুককর্পাসকে আংশিকভাবে একটি প্রশিক্ষণ ডেটাসেট হিসাবে বেছে নেওয়া হয়েছিল, কারণ ক্রমাগত পাঠ্যের দীর্ঘ প্যাসেজগুলি মডেলটিকে দীর্ঘ-পরিসরের তথ্য পরিচালনা করতে শিখতে সাহায্য করেছিল। এতে বিভিন্ন ঘরানার ৭,০০০এরও বেশি অপ্রকাশিত কথাসাহিত্যের বই ছিল। সেই সময়ে উপলব্ধ বাকি ডেটাসেটগুলি বড় হওয়ার পাশাপাশি দীর্ঘ-পরিসরের কাঠামোর অভাব ছিল (বাক্য স্তরে \"এলোমেলো\" হয়ে যেতো)।\nবুককর্পাস পাঠ্যটি এফটিএফওয়াই লাইব্রেরি দ্বারা প্রমিত বিরামচিহ্ন এবং হোয়াইটস্পেসে পরিষ্কার করা হয়েছিল এবং তারপরে স্প্যাসি দ্বারা টোকেনাইজ করা হয়েছিল।\n\n\n== স্থাপত্য ==\nজিপিট-১ স্থাপত্যটি ছিল একটি বারো-স্তর ডিকোডার-অনলি ট্রান্সফরমার, বারোটি মুখোশযুক্ত স্ব-মনোযোগ হেড ব্যবহার করে, যার প্রতিটিতে ৬৪-মাত্রিক অবস্থা ছিল (মোট ৭৬৮টির জন্য)। সাধারণ স্টোকাস্টিক গ্রেডিয়েন্ট ডিসেন্টের পরিবর্তে অ্যাডাম অপ্টিমাইজেশান অ্যালগরিদম ব্যবহার করা হয়েছিল; শেখার হার প্রথম ২,০০০ আপডেটে শূন্য থেকে সর্বোচ্চ ২.৫×১০−৪ পর্যন্ত রৈখিকভাবে বৃদ্ধি করা হয়েছিল এবং একটি কোসাইন সময়সূচী ব্যবহার করে ০-তে সংযুক্ত করা হয়েছিল। জিপিটি-১ এর ১১৭ মিলিয়ন প্যারামিটার রয়েছে।\nযদিও ফাইন-টিউনিং বিশেষ কাজের জন্য মানানসই করা হয়েছিল, কিন্তু তার পূর্ব প্রশিক্ষণ করা হয়নি। বিভিন্ন কাজ সম্পাদনের জন্য এর মৌলিক টাস্ক-অজ্ঞাত মডেল স্থাপত্যের উপর খুবই সামান্য পরিবর্তন করা হয়েছিল। তবুও, জিপিটি-১ পূর্ববর্তী মানদণ্ডগুলোর তুলনায় বেশ কিছু ভাষা প্রক্রিয়া সম্পর্কিত কাজে উন্নতি অর্জন করেছিল এবং বিভিন্ন ধরনের কাজে টাস্ক-ভিত্তিক স্থাপত্যের মাধ্যমে প্রশিক্ষিত মডেলগুলোকে ছাড়িয়ে গিয়েছিল।\n\n\n== কর্মক্ষমতা এবং মূল্যায়ন ==\nজিপিটি-১ প্রাকৃতিক ভাষা অনুসম্পর্ক (যা টেক্সচুয়াল এনটেইলমেন্ট নামেও পরিচিত) সম্পর্কিত কাজগুলিতে পূর্বের সেরা ফলাফলের তুলনায় ৫.৮% এবং ১.৫% উন্নতি অর্জন করেছিল, যা বিভিন্ন ডেটাসেট থেকে দুটি বাক্যের সম্পর্ক ব্যাখ্যা করে \"এনটেইলমেন্ট\", \"বিরোধিতা\" অথবা \"নিউট্রাল\" হিসেবে শ্রেণীবদ্ধ করার সক্ষমতা মূল্যায়ন করে। এরকম কিছু ডেটাসেটের মধ্যে রয়েছে কিউএনএলআই (উইকিপিডিয়া নিবন্ধ) এবং মাল্টি এনএলআই (রূপান্তরিত ভাষণ, জনপ্রিয় উপন্যাস, এবং সরকারি রিপোর্টসহ অন্যান্য উৎস)। তদ্রূপ, এটি পূর্ববর্তী মডেলগুলিকে ছাড়িয়ে গিয়েছিল প্রশ্নোত্তর এবং সাধারণ জ্ঞান যুক্তি সম্পর্কিত দুটি কাজে—রেস  (মধ্য এবং উচ্চ বিদ্যালয়ের পরীক্ষার লিখিত প্রশ্নোত্তরের ডেটাসেট) তে ৫.৭% এবং স্টোরি ক্লোজ টেস্টে ৮.৯% উন্নতি লাভ করেছিল।\nজিপিটি-১ পূর্ববর্তী সেরা মডেলগুলির চেয়ে ৪.২% বেশি উন্নতি করেছে সেমান্টিক সাদৃশ্য (অথবা সমার্থক বাক্য চিহ্নিতকরণ) কাজে, যেখানে দুটি বাক্য একে অপরের সমার্থক কিনা, তা নির্ধারণ করার সক্ষমতা পরীক্ষা করা হয়। এ জন্য কোরা কোশ্চেন পেয়ার্স (কিউকিউপি) ডেটাসেট ব্যবহার করা হয়েছে।\nজিপিটি-১ টেক্সট শ্রেণিবদ্ধকরণ কাজে কর্পাস অব লিঙ্গুইস্টিক অ্যাকসেপ্টেবিলিটি (কোলা) ব্যবহার করে ৪৫.৪ পয়েন্ট অর্জন করেছিল, যেখানে পূর্ববর্তী সেরা ছিল ৩৫.০। অবশেষে, জিপিটি-১ বহু-টাস্ক পরীক্ষা জিএলইউইতে মোট ৭২.৮ পয়েন্ট অর্জন করেছিল, যেখানে পূর্ববর্তী রেকর্ড ছিল ৬৮.৯।\n\n\n== তথ্যসূত্র =="}